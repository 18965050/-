codies autorebalance方案:
(1) 每个redis节点可依据配置不同分配不同的slot num
(2) (slotnum * memAlloc)*totalSlot/sum(slotnum * memAlloc)
https://github.com/wandoulabs/codis/blob/master/cmd/cconfig/rebalancer.go#L104


(1) 一致性哈希算法和普通hash求余算法(codes使用后者--预分配hash求余)
一致性hash:
    优点: 单向性, 数据迁移少
    缺点: 但节点删除或增加时, 会导致数据分布不均匀. 一致性hash提出了平衡性的概念, 即每个真实节点可以有多个虚拟节点(复制个数), 虚拟节点也计算hash,对对应
          到一个真实节点上, 数据通过虚拟节点存储到对应的真实节点. 这样能比较好的解决数据分配的问题.



(2)预分配槽数(slot)hash求余: hash(key)%slot
预先分配一定的slot, 根据节点数计算每个节点的hash范围. 当节点变化时, 重新计算每个节点对应的slot范围, 并做相应数据的迁移
    优点: 数据相对均衡, 实现简单
    缺点: (a)数据迁移量大
          (b)当节点数>slot数时, 会导致有些节点分配不到slot


代理跳转还是复制:
(1) 缓存复制会碰到很多麻烦的问题:　比如各节点数据一致性(CAP中的C),复制节点挂掉等, 性能考虑等等. 另外, 我们已经用数据源存储数据了,是否再必要缓存点间的相互复制
 (2) redis已经提供了复制功能(snapshot, aof)
 代理就简单多了,


还有个问题:
不能像单点redis一样, 支持所有redis操作命令, 需说明哪些命令不支持, 为什么

迁移的时候是根据slot进行迁移

如果某个slot状态为offline, server-proxy操作时进行timeout及retry处理

要注意redis key的类型

读写一致性问题考虑

redis-protocol: http://redis.io/topics/protocol (英文)
                http://www.redis.cn/topics/protocol.html (中文)
                https://github.com/spullara/redis-protocol.git (实现)


组件:
jedis-proxy: 对jedis的包装, 通过配置建立对某个namespace下所有server-proxy集群的连接
zookeeper: 协调管理器, 负责对每个server-proxy进行管理
server-proxy: 每个server-proxy对应一个redis集群, 需要实现redis protocol(RESP:REdis Serialization Protocol), 便于client api在不修改
              的情况下使用. 每个server-proxy包含若干个group, 每个group拥有一个master和若干个slave


#组件:
#server-proxy: 中心节点,集群配置, 需要避免单点部署, 相互无状态
#          (1) 对jedis-proxy: 解析jedis-proxy操作命令, 将处理转发到相应的redis-proxy上, 获取结果并返还给jedis-proxy
#          (2) 对redis-proxy: 分配槽点范围,hash key 并转发到相应的redis-proxy上
#          (3) 检测redis-proxy心跳
#          (4) 检测数据源是否可用
#jedis-proxy: redis客户端jedis的替代品, 提供大部分redis操作命令
#redis-proxy: 对redis master和slave节点进行包装, 提供
#             (1) redis存储操作
#             (2) 数据持久化到数据源的操作
#             (3) master或slave节点宕机处理
#             (4) sharding和resharding处理

server-proxy启动流程: server-proxy必须是无状态的
(1) 连接zk集群.判断/cache/namespace/server 目录是否存在
     不存在, 创建此目录,并对此目录添加NodeChildrenChanged watch
     存在, 直接对此目录添加 NodeChildrenChanged watch
(2) 在/cache/namespace/server下创建sequence Ephemeral index节点, 写入ip, port, status=[INIT]信息
(3) server-watch被触发,
    判断当前server-proxy,判断server的status是否为INIT.
    是, 等待直到status为PRE-MIGRATING
    否, 判断当前节点状态:
        PRE-MIGRATING,STARTED: 判断是否存在MIGRATING的其他server-proxy节点
            是: 如果为PRE-MIGRATING直接设置server-proxy STATUS=STARTED, 否则什么也不做
            否: 判断是否有slot的status=OFFLINE:
                是: 执行sharding流程. 所有数据完成后, 设置server-proxy STATUS=STARTED
                否: 如果为PRE-MIGRATING直接设置server-proxy STATUS=STARTED, 否则什么也不做
        MIGRATING:不做任务处理

(4) 判断数据源检测定时任务是否存在, 不存在则从配置文件加载数据源配置, 创建数据源连接, 启动定时任务, 检测各数据源是否可用.
    如可用, 判断/cache/namespace/db/index目录是否存在. 如果不存在,写入zk /cache/namespace/db目录下, 子目录为index(比如1,2,等),子节点信息包括为:
    type,address,database,parameters, username, password
    如重试不可用, 删除对应的子目录. 也可考虑对zk /cache/namespace/db加节点变更watch, 直接通知其他watch节点
(5) 根据配置文件判断/cache/namespace/group 目录是否存在,如不存在创建之, 并对此目录添加NodeChildrenChanged watch(group-watch)
    获取此namespace下所有的group, group master, group slaves, 获取所有的redis节点, 建立连接,并定时检测处理
    /cache/namespace/group/index,
    /cache/namespace/group/index/master, 添加NodeDeleted watch
    /cache/namespace/group/index/slave, 添加NodeChildrenChanged watch
    /cache/namespace/group/index/slave/index 节点,
    并对/cache/namespace/group, /cache/namespace/group/index, /cache/namespace/group/index/slave 目录添加 NodeChildrenChanged watch
从zk中读取 /cache/namespace/group_x以及 /cache/namespace/group_x/master, /cache/namespace/group_x/slave, /cache/namespace/group_x/down等节点信息
    获取分组, 每个分组中的master, slave, down节点信息
(6) 定时任务建立对master, slave, down节点的连接, 并定时检测处理

    启动定时任务, 检测各个redis节点是否可用
(7) 设置STATUS=PRE-MIGRATING


sharding流程:
(1) 设置server-proxy节点状态为MIGRATING
(2) 遍历每个slot, 对状态为OFFLINE的槽点进行还原操作. 还原过程见下
(3) 获取所有可用的master redis节点, 计算每个master节点的slot范围.
(4) 从slot 1槽点开始,顺序获取待处理的槽点
(5) 设置slot 1槽点状态为[OFFLINE]
(6) 判断key的对应的新老master节点是否为同一个节点, 如果是, 不处理; 否则
(7) 在slot对应的老的master节点中,判断此key是否存在:
    (a) 存在情况下, 先判断key的类型, 依据不同类型将key及value移植到新的master节点中, 并持久化, 在老的master节点删除
    (b) 不存在, 从db中依据key类型进行加载到新的master节点中
(8) 此slot中的所有key处理结束后, 将slot写入新的master节点中, 并从老的节点中删除
    /cache/namespace/group/slot/1
(9) 设置slot 1槽点状态为[ONLINE]

slave 节点宕机流程
(1) 将此节点从/cache/namespace/group/slave删除, 并放入/cache/namespace/group/down节点下, 存放ip,port, source(代表由master节点还是slave节点变成不可用节点的)

master节点宕机流程
(1) 将此节点从/cache/namespace/group/master删除, 并放入/cache/namespace/group/down节点下
(2) 遍历此group下所有slave节点, 通过info命令查看每个db的key数量计算每个slave节点的key总数,找到总数最大的slave节点, 并将此节点从/cache/namespace/group/slave删除, 并放入/cache/namespace/group/master节点下
(3) 运行slaveofNoOne命令使其变为主节点
(4) 其他slave节点运行slaveOf命令变成此节点的slave节点? 这里需要验证资源使用是否可以

resharding流程(添加/删除group)
(1)


缓存操作过程
(1) client api连接到server-proxy
(2) server-proxy只有在MIGRATING或STARTED状态下才可以进行缓存操作
(3) server-proxy 对key进行hash取余, 得到对应的slot, 并找到此slot对应的group master节点
(4) 如果server-proxy状态为STARTED状态, 找到后直接跳转进行操作
(5) 如果sever-proxy状态为MIGRATING,
    判断key对应的槽点状态是否为OFFLINE, 是的话依据配置文件中slot.timeout和slot.retries等待此槽点状态变为ONLINE,并进行跳转操作.
    如果key对应的槽点状态是否为ONLINE, 直接跳转操作



从数据源中获取每个槽点的数据, 加载到不同的master redis节点上




#server-proxy启动流程:
#(1) 设置STATUS=INIT
#(2) 对每个namespace维护状态表(OFF: 这个namespace没有对应的redis cluster; MIGRATING: 数据正在迁移; MIGRATED: 数据迁移完成).
#    初始如果有namespace, 则每个namespace的状态为off
#(2) 连接zk集群.创建/cache/meta 临时(Ephemeral)节点, 将server-proxy的ip:port信息写入, 当server-proxy关闭(正常或异常)时, 此节点会被删除
#(3) 从配置文件加载数据源配置, 创建数据源连接, 启动定时任务, 检测各数据源是否可用.
#    如可用, 写入zk /cache/db目录下, 子目录为index(比如1,2,等),子节点信息包括为:
#    type,address,database,parameters, username, password
#    如重试不可用, 删除对应的子目录
#(4) 获取/cache/namespace目录子节点, 对每个namespace在namespace状态表中添加条目{namespace:OFF}
#(5) 如果此条目下拥有group及group master节点,判断每个namespace中每个group的master节点状态是否为ONLINE状态,
#    如果均为ONLINE状态, 修改namespace状态表{namespace:MIGRATED}
#    否则修改namespace状态表{namespace:MIGRATING}
#(5) 设置STATUS=STARTED


jedis-proxy 启动流程:
(1) 连接server-proxy集群,对集群中的每个点获取连接通道
(2) 连接通道选择策略可选: roundrobin, random, failover
(3) 将缓存操作转换为request command, 发送给server-proxy, 并等待响应

master redis-proxy启动流程
(1) 设置 status=INIT
(2) 判断是否能连接对应的redis
(3) 到server-proxy中注册(REGISTER),带入namespace, group, master及对应redis的地址信息
(4) server-proxy进行校验,判断:
    (a) 如果是master节点, 判断zk是否存在/cache/namespace/xxx/groupName/master节点,如果是, 则不允许一个group下有多个master节点
    (b) 如果是slave节点, 判断zk是否存在/cache/namespace/xxx/groupName/master节点,如果否,则不允许master节点不在时, 直接挂载slave节点
    (c) 如果是slave节点, 判断/cache/namespace/xxx/groupName/slave目录下的节点没有注册redis的信息
(5) 如果是master节点, /cache/namespace/xxx/groupName下创建master节点, 数据为注册redis地址信息
    如果是slave节点, /cache/namespace/xxx/groupName/slave/index下创建slave节点, 数据为注册redis地址信息
(6) 如果是slave节点, server-proxy返回对应group下master节点的redis信息
    redis-proxy收到此信息后, 执行jedis的slaveOf命令
(7) 如果是master节点,server-proxy启动resharding流程
(8) 设置 status=ONLINE

slave redis-proxy启动流程
见上

缓存操作流程

master redis 宕机流程

slave redis 宕机流程

master redis-proxy宕机流程

slave redis-proxy宕机流程

namespace中添加/删除节点 resharding流程
(1) server-proxy修改对应的namespace状态为MIGRATING
(2) server-proxy重新计算这个namespace下每个master节点对应的槽数范围, 并记录到/cache/namespace/xxx/groupName/master/slots节点上, 数据格式为数组形式, 比如[1,2,3]
(3) 通知每个redis-proxy,


